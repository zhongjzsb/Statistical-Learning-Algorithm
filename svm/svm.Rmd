---
title: "Support Vector Machine"
author: Jingyu Bao
date: 2016-11-14
output: 
    html_notebook:
        theme: united
        highlight: tango
        css: faded.css
        toc: true
        self_contained: no
---

Mapping the data into another space by phi function, try to seperate them as much as possible. And the kernel function which $K(x, y) = < \phi(x), \phi(y) >$ makes the computation much easier.
[Kernel Explanation](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)


## Simple Usage

We use the segmentationData in __caret__ package. The Class is the predict variable (y label).


```{r}
data(segmentationData)

segmentationData %>% 
    head(10)

set.seed(100)
trainIndex <- createDataPartition(segmentationData$Class, p = .8, 
                                  list = FALSE, 
                                  times = 1)

segTrain <- segmentationData[trainIndex,]
segTest <- segmentationData[-trainIndex,]
trainX <- segTrain[,4:61]

set.seed(1000)
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated five times
                           repeats = 5)

svmFit <- train(x = trainX, 
                y = segTrain$Class,
                data = segTrain,
                method = "svmLinear", 
                trControl = fitControl)

svmFit

svmFit$finalModel
```

C is the penalty parameter of the error term. In this linear kernel function case, C remains $1$.

Predict/Test
```{r}
segPred <- predict(svmFit, segTest[,4:61])

segPred %>% 
    head(10)

confusionMatrix(data = segPred, reference = segTest$Class)

postResample(pred = segPred, obs = segTest$Class)
```




For Kappa statistic, from [here](http://stats.stackexchange.com/questions/82162/kappa-statistic-in-plain-english)

> Kappa = (observed accuracy - expected accuracy)/(1 - expected accuracy)

> There is not a standardized interpretation of the kappa statistic. According to Wikipedia (citing their paper), Landis and Koch considers 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect. Fleiss considers kappas > 0.75 as excellent, 0.40-0.75 as fair to good, and < 0.40 as poor.


## Next Step (Theory)

The RBF kernel 
$$
K(x,y) = \exp \Big(\frac{||x-y||^2}{\gamma}\Big) = \exp(||x||^2/\gamma)\exp(||y||^2/\gamma) \sum_{j = 0}^{\infty} \frac{(-2x\cdot y)^j}{\gamma^j j!}
$$
is a similarity measure which project the variables into infinity space $\mathbb{R}^{\infty}$.

from [here](./svm guide.pdf)

> In general, the RBF kernel is a reasonable first choice. This kernel nonlinearly maps
samples into a higher dimensional space so it, unlike the linear kernel, can handle the
case when the relation between class labels and attributes is nonlinear. Furthermore,
the linear kernel is a special case of RBF

> If the number of features is large, one may not need to map data to a higher dimensional
space. That is, the nonlinear mapping does not improve the performance.
Using the linear kernel is good enough, and one only searches for the parameter C.

## Visualization

```{r}
library(e1071)
data(cats, package = "MASS")
m <- svm(Sex~., data = cats)
plot(m, cats)

svmRadialFit <- train(Species ~ .,
                data = iris, 
                method = "svmRadial", 
                metric = "Kappa",
                control = trainControl(method = "cv"))


plot(svmRadialFit, metric = svmRadialFit$metric)

ggplot(svmRadialFit) + theme_bw()
```



### Others


example from [here](https://www.r-bloggers.com/the-5th-tribe-support-vector-machines-and-caret/)

```{r}
## SUPPORT VECTOR MACHINE MODEL
# First pass
set.seed(1492)
# Setup for cross validation
ctrl <- trainControl(method="repeatedcv",   # 10fold cross validation
                     repeats=2,		    # do 2 repititions of cv
                     summaryFunction=twoClassSummary,	# Use AUC to pick the best model
                     classProbs=TRUE)
 
 
#Train and Tune the SVM
svm.tune <- train(x =trainX,
                  y = segTrain$Class,
                  method = "svmRadial",   # Radial kernel
                  tuneLength = 9,					# 9 values of the cost function
                  preProc = c("center","scale"),  # Center and scale data
                  metric = "ROC",
                  trControl = ctrl)
svm.tune
```


Another example from [here](http://machinelearningmastery.com/non-linear-regression-in-r/)

```{r}
# load the package
library(kernlab)
# load data
data(longley)
# fit model
fit <- ksvm(Employed~., longley)
# summarize the fit
summary(fit)
# make predictions
predictions <- predict(fit, longley)
# summarize accuracy
rmse <- mean((longley$Employed - predictions)^2)
print(rmse)
```

